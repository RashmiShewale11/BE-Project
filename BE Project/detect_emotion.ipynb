{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "0 573300\n",
      "[0.         0.         0.         ... 0.00021359 0.00019978 0.00022871]\n"
     ]
    }
   ],
   "source": [
    "sr = 44100\n",
    "audio_vectors = {}\n",
    "wav_file_path = '/home/rashmi/Dataset/pre-processed/single_file/test_wav/ses05m_impro06_M004.wav'\n",
    "\n",
    "orig_wav_vector, _sr = librosa.load(wav_file_path, sr=sr)\n",
    "orig_wav_vector, _sr\n",
    "\n",
    "start_time = 0\n",
    "end_time = 13\n",
    "start_frame = math.floor(start_time * sr)\n",
    "end_frame =  math.floor(end_time * sr)\n",
    "\n",
    "print (orig_wav_vector)\n",
    "print (start_frame,end_frame)\n",
    "\n",
    "truncated_wav_vector = orig_wav_vector[start_frame:end_frame + 1]\n",
    "print (truncated_wav_vector)\n",
    "\n",
    "truncated_wav_file_name= \"Ses05F_impro08\"\n",
    "audio_vectors[truncated_wav_file_name] = truncated_wav_vector\n",
    "\n",
    "with open('/home/rashmi/Dataset/pre-processed/single_file/audio_vectors.pkl', 'wb') as f:\n",
    "        pickle.dump(audio_vectors, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import math\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_vectors_path = '/home/rashmi/Dataset/pre-processed/single_file/audio_vectors.pkl'\n",
    "audio_vectors = pickle.load(open(audio_vectors_path, 'rb'))\n",
    "random_file_name = list(audio_vectors.keys())[random.choice(range(len(audio_vectors.keys())))]\n",
    "y = audio_vectors[random_file_name]\n",
    "sr = 44100\n",
    "\n",
    "feature_list = [random_file_name];\n",
    "sig_mean = np.mean(abs(y))\n",
    "feature_list.append(sig_mean)  # sig_mean\n",
    "feature_list.append(np.std(y))  # sig_std\n",
    "\n",
    "rms = librosa.feature.rms(y + 0.0001)[0]\n",
    "feature_list.append(np.mean(rms))  # rms_mean\n",
    "feature_list.append(np.std(rms))  # rms_std\n",
    "\n",
    "silence = 0\n",
    "for e in rms:\n",
    "    if e <= 0.4 * np.mean(rms):\n",
    "        silence += 1\n",
    "silence /= float(len(rms))\n",
    "feature_list.append(silence)  # silence\n",
    "\n",
    "y_harmonic = librosa.effects.hpss(y)[0]\n",
    "feature_list.append(np.mean(y_harmonic) * 1000)  # harmonic (scaled by 1000)\n",
    "\n",
    "# based on the pitch detection algorithm mentioned here:\n",
    "# http://access.feld.cvut.cz/view.php?cisloclanku=2009060001\n",
    "cl = 0.45 * sig_mean\n",
    "center_clipped = []\n",
    "for s in y:\n",
    "    if s >= cl:\n",
    "        center_clipped.append(s - cl)\n",
    "    elif s <= -cl:\n",
    "        center_clipped.append(s + cl)\n",
    "    elif np.abs(s) < cl:\n",
    "        center_clipped.append(0)\n",
    "auto_corrs = librosa.core.autocorrelate(np.array(center_clipped))\n",
    "feature_list.append(1000 * np.max(auto_corrs)/len(auto_corrs))  # auto_corr_max (scaled by 1000)\n",
    "feature_list.append(np.std(auto_corrs))  # auto_corr_std\n",
    "\n",
    "columns = ['wav_file', 'sig_mean', 'sig_std', 'rmse_mean', 'rmse_std', 'silence', 'harmonic', 'auto_corr_max', 'auto_corr_std']\n",
    "df_features = pd.DataFrame(columns=columns)\n",
    "\n",
    "df_features = df_features.append(pd.DataFrame(feature_list, index=columns).transpose(), ignore_index=True)\n",
    "\n",
    "df_features.to_csv('/home/rashmi/Dataset/pre-processed/single_file/audio_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1704, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv('/home/rashmi/Dataset/pre-processed/s2e/audio_train.csv')\n",
    "#x_test = pd.read_csv('/home/rashmi/Dataset/pre-processed/s2e/audio_test.csv')\n",
    "\n",
    "print(x_train.shape)\n",
    "y_train = x_train['label']\n",
    "#y_test = x_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1704, 10)\n",
      "{2: 508, 1: 357, 5: 297, 3: 242, 4: 165, 0: 135}\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)#, x_test.shape)\n",
    "cl_weight = dict(pd.Series(x_train['label']).value_counts(normalize=True))\n",
    "print(dict(pd.Series(x_train['label']).value_counts()))\n",
    "\n",
    "del x_train['label']\n",
    "#del x_test['label']\n",
    "del x_train['wav_file']\n",
    "#del x_test['wav_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sig_mean   sig_std  rmse_mean  rmse_std   silence  harmonic  auto_corr_max  \\\n",
      "0  0.003807  0.006724   0.004809    0.0047  0.465179 -0.008355       0.034514   \n",
      "\n",
      "   auto_corr_std  \n",
      "0       0.189598  \n"
     ]
    }
   ],
   "source": [
    "x_test_new=pd.read_csv('/home/rashmi/Dataset/pre-processed/single_file/audio_features.csv')\n",
    "del x_test_new['wav_file']\n",
    "print (x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {'ang': 0,\n",
    "                'hap': 1,\n",
    "                'sad': 2,\n",
    "                'fea': 3,\n",
    "                'sur': 4,\n",
    "                'neu': 5}\n",
    "\n",
    "emo_keys = list(['ang', 'hap', 'sad', 'fea', 'sur', 'neu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=1200, min_samples_split=25)\n",
    "xgb_classifier = xgb.XGBClassifier(max_depth=7, learning_rate=0.008, objective='multi:softprob', \n",
    "                                   n_estimators=1200, sub_sample=0.8, num_class=len(emotion_dict),\n",
    "                                   booster='gbtree', n_jobs=4)\n",
    "mnb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06853257 0.26746664 0.37977596 0.06861857 0.0723886  0.14321766]]\n"
     ]
    }
   ],
   "source": [
    "eclf1 = VotingClassifier(estimators=[('rf', rf_classifier), ('xgb', xgb_classifier), ('mnb', mnb_classifier)], voting='soft')\n",
    "eclf1 = eclf1.fit(x_train, y_train)\n",
    "pred_probs=eclf1.predict_proba(x_test_new)\n",
    "print (pred_probs)\n",
    "\n",
    "#np.savetxt('/home/rashmi/Dataset/pre-processed/emo_val/rf.csv',pred_probs,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pred_probs.argmax(axis=1).item()\n",
    "index\n",
    "emo_keys[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sad'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
